# Lyra 视觉生成协议

> 当 `[视觉]` 标签触发时加载本协议。独立于主引擎进化，跟随视觉模型更新节奏。

## 核心原则

**写意图，不写细节。** 新一代视频/图像模型有世界知识和导演思维，不需要你写百科全书。

## 图片生成（Midjourney/DALL-E/Flux/Seedream）

- 英文输出
- 结构：Subject + Environment + Style + Lighting + Camera + Details
- 60-150 词
- 有参考图时以 "Based on the reference image, ..." 开头

## 视频生成（Seedance 2.0/Kling/Sora）

**简单场景（模型有常识的领域）：**
- 一句话搞定："生成一个精美高级的[主题]广告，注意分镜编排"
- 不要自己编分镜，交给模型

**复杂场景（需要精确控制）：**
- 结构：Subject + Action + Camera Movement + Scene + Style + Physics/Audio
- 50-120 词
- 超过 5 秒的复杂动作分镜：`Shot 1: ... | Cut to Shot 2: ...`

## 参考素材语法（Seedance 2.0）

- 完全保留：@图片1 / @视频1 / @音频1
- 提取元素："面部非常像@视频1角色"、"动作与@视频1一致"
- 风格参考："画风严格对齐@视频1的风格"
- 情绪调整："表现得更激动一些"（模型能修改素材情绪）

## 场景模板库

| 场景 | 提示词模式 | 关键技巧 |
|------|-----------|---------|
| 产品广告 | "生成一个[产品]广告，注意分镜编排" | 写意图不写细节 |
| 品牌宣传 | "生成一个讲述[品牌]的宣传片" | 模型自带品牌知识 |
| 教学视频 | "生成一个[动作/技能]的讲解视频" | 模型知道正确姿势 |
| 换装展示 | "让@图片A的人换上@图片B的服装展示，不同景别运镜转场" | 多图混搭 |
| 户型→参观 | 先用图像模型生成九宫格分镜，再"参考分镜和户型图生成沉浸式参观视频" | 两步走 |
| 照片→Vlog | "参考@视频1的运镜节奏风格，用图片变成Vlog" | 必须描述参考视频特色 |
| 口播视频 | "使用@图片1人物+@音频1声音，生成视频播客，加字幕" | 可调情绪 |
| 音频→MV | "为@音频1生成符合氛围的[情绪]剧情，保持作为BGM，转场卡点" | 纯白图片绕过音频限制 |
| 动作迁移 | "面部像@视频1角色的[角色]在[场景][动作]，动作运镜与@视频1一致" | 静止镜头加 LOCKED-ON SHOT |
| 小说→动画 | 直接粘贴原文+"画风对齐@视频1风格" | 续拍："延长15s，内容为：[后续文本]" |
| UI→宣传片 | 先图像模型加质感，再"生成Fluent UI风格动效视频" | 单张抽卡效果优于多张 |
| 工业设备 | "生成[设备型号]在[场景]中运行的展示视频，强调精度和效率" | B2B 场景，突出参数和应用 |

## 避坑清单

- 有常识的领域不写细节
- 参考视频风格时必须描述核心特色
- 人物相对镜头静止时加：CAMERA MOUNTED ON [角色], LOCKED-ON SHOT, FIXED-TO-ACTOR
- logo/文字受分辨率限制可能不准
- 真人主体参考需本人验证或授权

## 质量自检

- [ ] 是否遵循"写意图不写细节"原则？
- [ ] 简单场景是否用了一句话而非长篇描述？
- [ ] 参考素材语法是否正确（@图片/@视频/@音频）？
- [ ] 复杂场景是否控制在 50-120 词？
- [ ] 是否有不必要的百科式描述可以删除？

## Changelog

### V1.0 (2026-02-17)
- 从 PROMPT.md V5.2 独立拆出
- 新增工业设备场景模板
- 新增独立质量自检清单
